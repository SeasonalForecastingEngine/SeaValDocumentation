# Data import and processing

```{r setup, echo = F, message = F}
knitr::opts_chunk$set(fig.width = 5)
knitr::opts_chunk$set(fig.height = 6)
library(SeaVal)
library(ggpubr)
library(ggplot2)
load(file = '_data_dir.RData')
```



## Reshaping data {#data-examples}

For forecast validation, the ideal data format is to have all your fore- and hindcasts in the same data table, alongside the corresponding observations. So one column of forecasts, one column of observations and several columns of dimension variables (e.g. year, month, lon,lat). However, this is rarely how your netcdf-data looks like: you'll often have different netcdfs for observations and fore-/hindcasts. They might, moreover have different units, different missing values, different variable names etc.

So to get your data into the preferred shape, you either need to manipulate the netcdf files beforehand, to get exactly the data table you want from `netcdf_to_dt`, or you can extract several data tables and do the required manipulations in `R`, using `SeaVal` and `data.table`. In this section we show a few examples how to do this.

### Example: cross-validation data {#cv-data}

We first consider a crossvalidation dataset, containing predictions for the February-April rainfall. 

```{r}
# get the two CV-files:
fn_pred = "CrossValidatedPredictedRain_Feb-Apr_Feb2021.nc"

dt_pred = netcdf_to_dt(paste0(data_dir,fn_pred)) # here data_dir is the local path to the data (not shown)

# let's look at the data:
print(dt_pred)
# it's also always good to have a look at plots:

dt_plot = dt_pred[time == 13] # subset to only one time-slize, so that we can plot a map
ggplot_dt(dt_plot,'prec')

```

Next we need observations to compare the predictions to. For this first example, we have a prepared netcdf with the observations available:
```{r}
fn_obs = "ObservedRain_Feb-Apr_Feb2021.nc"
dt_obs = netcdf_to_dt(paste0(data_dir,fn_obs))
```

Now we have two data tables, one with predictions and one with observations. We want to merge them together. There are two main functions for merging data tables: `rbindlist` and `merge`. The function `rbindlist` takes several data tables, all with the same columns, and just appends them. This is appropriate when your data tables contain *the same type of information, but for different coordinates*. For example, if you have a data table with observed precipitation for 1990-2020 and a second data table with observed precipitation for 2021-2022. In our example we have two data tables but they contain *different types of information* (the first contains predictions, the second observations). However, they contain them at the same coordinates (meaning at the same locations specified by `lon`/`lat`, and for the same months specified by `time`). Such data tables are joined by the function `merge`. Before we join predictions and observations, we should rename their columns, though: Both of them currently have a column called `prec`, which contains the observations in `dt_obs` and the predictions in `dt_pred`.
```{r}
setnames(dt_pred,'prec','prediction')
setnames(dt_obs,'prec','observation')

dt = merge(dt_pred,dt_obs,
           by = c('lon','lat','time'))
print(dt)
```
The `by`-argument of the `merge` function identifies the columns that are present in both data tables. These are typically the columns specifying coordinates. 
We now have prediction and observation side by side as we wanted. However, we see that also coordinates with missing values are dragged along (where both the prediction and the observation is `NA`). Let's remove them:
```{r}
dt = dt[!is.na(prediction)]
```

Also, the `time` column is not really easy to read. Looking at the information printed out by `netcdf_to_dt` above tells us that the unit of time in the netcdf is months since 1981-01-01. `SeaVal` has a function called `MSD_to_YM` that converges time from this 'months-since-date' (MSD) format to years and months (YM), which is typically much more useful:
```{r}
dt = MSD_to_YM(dt,
               timecol = 'time', # what is the column called that contains the time-since-date coordinates?
               origin = '1981-01-01') # (the origin was documented in the netcdf, see above.)
print(dt) 
```

Now the data table looks much better and contains prediction and observation side by side. The column ordering is a bit weird with predictions and observations in the middle, but this usually should not matter.  In section \@ref(cv-eval) we continue this cross-validation example and show how to evaluate the predictions.

```{r,echo = FALSE}
# for use in the next section:
dt_cv = copy(dt)
```


### Example: Tercile forecasts and upscaling {#us-obs}

In the first example we were lucky that our observations and predictions were of the same format, which is not always the case. We will now look at a more complex example where we prepare a typical tercile forecast for precipitation. We consider again the 2021 MAM season as example. Let's get started with collecting the prediction and corresponding observations:

```{r,cache = TRUE }
# predictions:
dt = netcdf_to_dt(paste0(data_dir,'PredictedProbabilityRain_Mar-May_Feb2021_new.nc'),
                  verbose = 0) # suppresses that the function prints all the netcdf-information
print(dt)

# observation:
dt_obs = netcdf_to_dt(paste0(data_dir,'ObservedChirpsRainTotal_MAM2021.nc'),
                      vars = 'precip', # The netcdf file contains multiple variables, 
                                       # we only want precip.
                      verbose = 1) # This option only prints the units, which are usually the most important part:
print(dt_obs)
```

As we can see, the observation data table looks quite different now. For example, the lon/lat columns are called `longitude` and `latitude`. If we want to visualize the data with the `ggplot_dt` function, we need to rename them:
```{r}
setnames(dt_obs,c('longitude','latitude'),c('lon','lat'))
```

Let's now look at maps of the prediction and observation side by side
 

```{r, warning=FALSE}
pp1 = ggplot_dt(dt_obs,'precip') + ggtitle('Observed MAM prec. 2021')
pp2 = ggplot_dt(dt,'above') + ggtitle('Predicted probability of wet season')

ggpubr::ggarrange(pp1,pp2)
```

We have two problems: First, observations and predictions are on a different spatial scale making it impossible to compare them directly. Second, the observation is only given as mm/month, and the prediction are probabilities for climatology-terciles. Therefore, we need to establish a local climatology first which requires collecting past rainfall observations. Only thereafter we can derive in which climatology-tercile the 2021 observation falls.

Lets first get observations and predictions onto the same scale. For validation, it is usually more appropriate to upscale everything to the coarser scale.
To this end we use the upscaling function `upscale_regular_lon_lat`

```{r}
dt_obs = upscale_regular_lon_lat(dt_obs,
                                 coarse_grid = dt,  # to which grid do you want to upscale?
                                 uscols = 'precip') # which column contains the data for upscaling?

pp3 = ggplot_dt(dt_obs,'precip') + ggtitle('Upscaled MAM prec. 2021')
ggpubr::ggarrange(pp3,pp2)
```

Now, let us address the second problem, namely assessing where the precipitation was above or below normal. The observation we loaded contains the CHIRPS MAM average for 2021. As we showed in Section \@ref(chirps), the `SeaVal` package provides convenient functions for locally storing CHIRPS data and loading it. So we can load a 30-year reference period like this:
```{r}
dt_past_obs = load_chirps(years = 1990:2020, months = 3:5)
print(dt_past_obs)
```

As we can see, the past observations are monthly, but we want the MAM season total. Moreover, the local CHIRPS data is stored in mm/day. So let us convert the units and sum the precipitation over the season:

```{r}
dt_past_obs[,prec := 30*prec] # conversion to mm
dt_past_obs = dt_past_obs[,.(precip = sum(prec)), by = .(lon,lat,year)]
```

Now `dt_past_obs` and `dt_obs` look very similar and we can join them together:
```{r}
dt_obs[,year := 2021]
dt_obs = rbindlist(list(dt_past_obs,dt_obs), 
                   use.names = TRUE) 
```

Now we can derive into which local climatology-tercile the precipitation of 2021 falls, by comparing with past observations. To this end we use the utility-function `add_tercile_cat`. The function establishes a climatology, and calculates into which tercile category each value falls: 

```{r}
dt_obs = add_tercile_cat(dt_obs, 
                         by = c('lon','lat'), # calculate the climatology and category separately for each location
                         datacol = 'precip')
print(dt_obs)
ggplot_dt(dt_obs[year == 2021],'tercile_cat',low = 'red',high = 'blue')
```

As we can see, the observation now contains a column `tercile_cat` that tells us whether the rainfall at this location in a given year was below normal (-1), normal (0) or above normal (1). For later use, we also add the local climatology:
```{r}
dt_obs[,clim := mean(precip),by = .(lon,lat)]
```

Finally, we can merge observations and predictions. We only need the observation for 2021 (now that we have derived the tercile category).
```{r}
dt = merge(dt,dt_obs[year == 2021],by = c('lon','lat'))
# transform percentage prediction to probabilities between zero and one:
dt[,normal := normal/100]
dt[,above := above/100]
dt[,below := below/100]

print(dt)
```

We continue to work with this dataset in Section \@ref(eval-terciles), where we evaluate this example prediction.
```{r,echo = F}
dt_tercile_forecast = copy(dt)
```

### Example: 'corrupted' netcdf{#ex-corrupted-netcdf}

Data handling can be messy and things can go wrong at any stage. Here, we have a look at a netcdf file where something has gone wrong:

```{r,error = TRUE}
fn = "PredictedProbabilityRain_Feb-Apr_Feb2021.nc"
dt = netcdf_to_dt(paste0(data_dir,fn))
```

The `netcdf_to_dt` function prints out the netcdf information, and then crashes with the error message above, saying that we have disjoint dimension variables for some variables. Indeed, looking at the printed out netcdf-description, we have three variables (below,normal,above), and while 'below' and 'above' are indexed by 'lon' and 'lat', 'normal' is indexed by 'ncl3' and 'ncl4'. As the error message suggests, we can set `trymerge` to FALSE, making `netcdf_to_dt` return a list of data tables, rather than a single data table. 

```{r}
dt_list = netcdf_to_dt(paste0(data_dir,fn),trymerge = FALSE,verbose = 0)
print(dt_list)
```

We see that 'ncl3' and 'ncl4' have different values than 'lon' and 'lat', apparently they are meaningless indexing integers. However, the three data.tables are of the same size, and we can hope that the 'below' data table is arranged in the same row-ordering than the others. If this is the case, we can simply extract the 'normal' column from it (as vector) and attach it to one of the others. Let's try:

```{r}

dt = dt_list[[1]]
normal_probs_as_vector = dt_list[[2]][,normal]
dt[,normal := normal_probs_as_vector]

ggplot_dt(dt,'normal')
```

Plotting is usually a great way to see whether data got arranged correctly:  Here, we can be fairly certain it did, simply because the missing values in the 'normal' vector are at the locations where they should be (over water and dry regions). If the ordering would have been differently, these would likely be all over the place. However, let's run another test to be certain:

```{r}
# attach the 'above'-data table:
dt = merge(dt,dt_list[[3]],by = c('lon','lat'))
print(dt)

# if the ordering of the 'normal' column was correct, we have below + normal + above = 100%:
check = rowSums(dt[,.(below,normal,above)])
print(check[1:20])
mean(check[!is.na(check)])
```

This shows that the ordering was correct. We also could have solved this differently, by using $\text{below} + \text{normal} + \text{above} = 100\%$ directly:

```{r}
# only extract 'below' and 'above':
dt = netcdf_to_dt(paste0(data_dir,fn), vars = c('below','above'),verbose = 0)
print(dt)
dt[,normal := 100 - below - above]
ggplot_dt(dt,'normal')
```



### Example: preparing data for evaluating exceedence probabilities{#data-ex-prexc}

Here we show how to prepare data for evaluating exceedence probabilities, see Section \@ref(eval-ex-pr).

```{r}
fn = 'PrecRegPeXcd_3monthSeasonal.nc'
dt = netcdf_to_dt(paste0(data_dir,fn))
print(dt)

# first, note that the 'model', 'rthr', and 'month' column do not make much sense before we insert
# the information we gather from the netcdf description above:
modelnames = c('GEM-NEMO',
               'CanCM4i',
               'NASA-GEOSS2S',
               'GFDL-SPEAR',
               'COLA-RSMAS-CCSM4',
               'NCEP-CFSv2',
               'ECMWF',
               'Meteo_France',
               'UKMO')
thresholds = c(200,300,350,400)

dt[,model := modelnames[model + 1]]
dt[,rthr := thresholds[rthr + 1]]
dt[,month :=lead + 2][,lead:=NULL]
```

Ultimately, we want to compare the skill of these models to a climatological forecast. The climatological forecast for the exceedence probability is just the fraction of observed years where the threshold was exceeded. To calculate this, we require past observations. So let us load chirps again, only for the months contained in `dt`:
```{r}
dt_chirps = load_chirps(months = unique(dt[,month]))
dt_chirps[,prec:=30*prec] # convert to mm
```

In order to get the climatological exceedence probabilities, we can use the following function:
```{r}
clim_fc = climatology_threshold_exceedence(dt_chirps,
                                           o = 'prec',
                                           thresholds = unique(dt[,rthr]),
                                           by = c('month','lon','lat'))

print(clim_fc)
```

Note that we passed the thresholds given in `dt`. The `by` argument tells the function what columns to group by when computing the climatology. Finally, we need to merge the predictions, the climatological forecast and the observation into one data table. Since we only have predictions for 2021, it is enough to provide the climatology forecast and observation for 2021 as well. Also note that we have predictions for more months than observations (at the time this is written), so we cut the predictions for June and July out - we cannot evaluate predictions we don't know the outcome for.

```{r}
setnames(clim_fc,c('pexcd','threshold'),c('clim','rthr'))
dt = merge(dt,clim_fc[year == 2021,],by = c('lon','lat','month','rthr'))
dt = merge(dt,dt_chirps[year == 2021],by = c('lon','lat','month','year'))

#finally, for evaluation we generally work with probabilities between 0 and 1, not percentages:
range(dt[,pexcd],na.rm = TRUE) # confirm that the data table contains percentages at the moment...
dt[,pexcd := pexcd/100] #... and correct

print(dt)
```
How to evaluate the predictions from here is discussed in Section \@ref(eval-ex-pr).

```{r,echo = FALSE}
dt_prexc = dt
save(dt_tercile_forecast,dt_cv,dt_obs,dt_prexc,file = '_temp.RData')
```
